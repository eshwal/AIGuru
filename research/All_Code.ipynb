{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "27b8c59d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from pinecone import Pinecone, ServerlessSpec,CloudProvider,VectorType,AwsRegion\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c8475f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents_from_pdf(directory_path):\n",
    "    \"\"\" Return a list of documents extracted from PDF files in the specifed directory.\"\"\"\n",
    "\n",
    "    loader = DirectoryLoader(\n",
    "        directory_path,\n",
    "        glob=\"**/*.pdf\",\n",
    "        loader_cls=PyPDFLoader\n",
    "    )\n",
    "\n",
    "    documents = loader.load()\n",
    "\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "333cf3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = load_documents_from_pdf(\"../data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "854cffca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks(documents):\n",
    "    \"\"\" Split the documents into smaller chunks for processing.\"\"\"\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\",\"\\n\"],\n",
    "        chunk_size = 200,\n",
    "        chunk_overlap = 100\n",
    "    )\n",
    "\n",
    "    chunks = splitter.split_documents(documents)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8fe953f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = split_into_chunks(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3f9a3f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4860"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d280b51b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2024-09-19T22:02:52+00:00', 'source': '..\\\\data\\\\Explorations in Artificial Intelligence and Machine Learning, CRC Press.pdf', 'total_pages': 178, 'page': 0, 'page_label': '1'}, page_content='Explorations in Artificial \\nIntelligence and M achine \\nLearning\\nA CRC Press FreeBook')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "122b4cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings():\n",
    "    \"\"\" Load the embeddings model for vectorization.\"\"\"\n",
    "\n",
    "    embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b5abd85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Esha\\GenAI\\Practice\\AIGuru\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Esha\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={} encode_kwargs={} query_encode_kwargs={} multi_process=False show_progress=False\n"
     ]
    }
   ],
   "source": [
    "print(load_embeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ba701aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings =load_embeddings()\n",
    "\n",
    "response = embeddings.embed_query(\"What is LLM?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b0c001c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0bd7a5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name=\"aibot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d272f713",
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "47491d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pinecone_index(index_name):\n",
    "    \"\"\" Create a Pinecone index for storing vector embeddings.\"\"\"\n",
    "\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "\n",
    "    if not pc.has_index(index_name):\n",
    "        pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,\n",
    "        spec=ServerlessSpec(\n",
    "        cloud=CloudProvider.AWS,\n",
    "        region=AwsRegion.US_EAST_1\n",
    "        ),\n",
    "        vector_type=VectorType.DENSE\n",
    "        )\n",
    "        print(f\"Index {index_name} created successfully.\")\n",
    "    else:\n",
    "        print(f\"Index {index_name} already exists.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bd9a0d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index aibot created successfully.\n"
     ]
    }
   ],
   "source": [
    "create_pinecone_index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b9a90ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4826/4826 [02:29<00:00, 32.31it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "vectors = []\n",
    "failed_indexes = []\n",
    "\n",
    "for i in tqdm(range(0, len(cleaned_texts))):\n",
    "    try:\n",
    "        vec = embeddings.embed_query(cleaned_texts[i])\n",
    "        vectors.append(vec)\n",
    "    except Exception as e:\n",
    "        failed_indexes.append((i, cleaned_texts[i], str(e)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9229d220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Total failed embeddings: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"❌ Total failed embeddings: {len(failed_indexes)}\")\n",
    "for idx, text, err in failed_indexes[:5]:\n",
    "    print(f\"[{idx}] Error: {err} | Text: {repr(text[:80])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f96c9065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error at index 1307: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]] | Text: 'we provide a quick overview of this crucial topic.\\nThe notat'\n"
     ]
    }
   ],
   "source": [
    "for i, text in enumerate(cleaned_texts):\n",
    "    try:\n",
    "        vec = embeddings.embed_query(text)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error at index {i}: {e} | Text: {repr(text[:60])}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "beb06c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "import unicodedata\n",
    "\n",
    "def clean_metadata(metadata: dict) -> dict:\n",
    "    clean_meta = {}\n",
    "    for key, value in metadata.items():\n",
    "        if isinstance(value, str):\n",
    "            clean_meta[key] = value.encode(\"utf-8\", \"ignore\").decode(\"utf-8\").strip()\n",
    "        else:\n",
    "            clean_meta[key] = value\n",
    "    return clean_meta\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove unprintable and invalid unicode characters\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Normalize and remove surrogates\n",
    "    try:\n",
    "        text = text.encode(\"utf-8\", \"replace\").decode(\"utf-8\")\n",
    "    except:\n",
    "        text = str(text)\n",
    "    \n",
    "    # Optionally remove control characters\n",
    "    text = ''.join(ch for ch in text if unicodedata.category(ch)[0] != 'C' or ch in '\\n\\t ')\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "cleaned_docs = []\n",
    "for doc in chunks:\n",
    "    if isinstance(doc.page_content, str) and len(doc.page_content.strip()) > 10:\n",
    "        cleaned_text = clean_text(doc.page_content)\n",
    "        cleaned_meta = clean_metadata(doc.metadata)\n",
    "        cleaned_docs.append(Document(page_content=cleaned_text, metadata=cleaned_meta))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "96961971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4826"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4eecebc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = PineconeVectorStore.from_documents(\n",
    "    documents=cleaned_docs,\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "911a92ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve documents from the vector store\n",
    "\n",
    "retrieved_docs = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a0bd1570",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = retrieved_docs.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "bc8460e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = '''\n",
    "    You are a helpful question answering assistant.\n",
    "    You will be provided with a question and you need to answer it based on the context provided.\n",
    "    If you don't know the answer or if the question is out of context,say \"I don't know.\"\n",
    "    Do not make up the answer or hallucinate. Also don't answer questions that are not related to the context provided.\n",
    "    Answer in maximum 3 sentences within the context  and keep it concise.\n",
    "    \\n\\n\n",
    "    {context}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "080fc87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", prompt),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d7b9a2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MISTRAL_API_KEY\"] = os.getenv(\"MISTRAL_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "9830c32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatMistralAI(\n",
    "    model_name=\"mistral-large-latest\",\n",
    "    max_tokens=500\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "54cd1ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "589f1096",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = create_stuff_documents_chain(llm=llm, prompt=final_prompt)\n",
    "rag_chain =create_retrieval_chain(retriever,qa_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "724cf6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'What is stats?', 'context': [Document(id='7fcfe7a8-4a5e-475f-8517-3066d62dbfa8', metadata={'creationdate': '', 'creator': 'PyPDF', 'page': 13.0, 'page_label': '14', 'producer': 'Skia/PDF m133 Google Docs Renderer', 'source': \"..\\\\data\\\\Generative AI_ A Beginner's Guide.pdf\", 'title': \"Generative AI: A Beginner's Guide\", 'total_pages': 62.0}, page_content='similar\\n \\nto\\n \\nthe\\n \\ndata\\n \\nthey\\n \\nwere\\n \\ntrained\\n \\non.\\n \\nThis\\n \\nallows\\n \\nthem\\n \\nto\\n \\nperform\\n \\na\\n \\nvariety\\n \\nof\\n \\nimpressive\\n \\ntasks,\\n \\nincluding:'), Document(id='b55a1a10-1d12-4c48-b577-882820549f81', metadata={'creationdate': '', 'creator': 'PyPDF', 'moddate': '2024-09-19T22:02:52+00:00', 'page': 24.0, 'page_label': '25', 'producer': 'Pdftools SDK', 'source': '..\\\\data\\\\Explorations in Artificial Intelligence and Machine Learning, CRC Press.pdf', 'total_pages': 178.0}, page_content='in the second example, we have much more data (the results of 100 tosses rather\\nthan 10) and so we should know more about r.'), Document(id='2f8cccc2-6579-4ad0-8b09-92d49d7f2d0a', metadata={'creationdate': '', 'creator': 'PyPDF', 'moddate': '2024-09-19T22:02:52+00:00', 'page': 131.0, 'page_label': '132', 'producer': 'Pdftools SDK', 'source': '..\\\\data\\\\Explorations in Artificial Intelligence and Machine Learning, CRC Press.pdf', 'total_pages': 178.0}, page_content='In Artiﬁcial Intelligence and Statistics, pages 448–455, 2009.\\n[52] Ruhi Sarikaya, Geoﬀrey E. Hinton, and Anoop Deoras. Application of')], 'answer': \"I don't know.\"}\n",
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What is stats?\"})\n",
    "print(response)\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "85bf5182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLPis a subfield of artificial intelligence (AI) that focuses on the interaction between computers and humans through natural language. The goal of NLP is to enable computers to understand, interpret, and generate human language in a way that is both meaningful and useful.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What is NLP?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "7aff166c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What is Acne?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "d83b876f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'What is headache meaning?', 'context': [Document(id='c45997a2-ca4b-4eaf-91bc-2efc22824c75', metadata={'creationdate': '', 'creator': 'PyPDF', 'moddate': '2024-09-19T22:02:52+00:00', 'page': 9.0, 'page_label': '10', 'producer': 'Pdftools SDK', 'source': '..\\\\data\\\\Explorations in Artificial Intelligence and Machine Learning, CRC Press.pdf', 'total_pages': 178.0}, page_content='that we label as intelligent by learning from experience. Learning is what gives us ﬂexibility\\nin our life; the fact that we can adjust and adapt to new circumstances, and learn new'), Document(id='172f5b51-f67e-4caf-a817-9b85f35ef1c1', metadata={'creationdate': '', 'creator': 'PyPDF', 'moddate': '2024-09-19T22:02:52+00:00', 'page': 172.0, 'page_label': '173', 'producer': 'Pdftools SDK', 'source': '..\\\\data\\\\Explorations in Artificial Intelligence and Machine Learning, CRC Press.pdf', 'total_pages': 178.0}, page_content='between a feeling and a thought is. Feeling pain and knowing about pain \\nare certainly not the same internal states. I am hopeful that future research'), Document(id='d23aebae-ae91-4b94-965f-0fb2c2e06fab', metadata={'creationdate': '', 'creator': 'PyPDF', 'moddate': '2024-09-19T22:02:52+00:00', 'page': 172.0, 'page_label': '173', 'producer': 'Pdftools SDK', 'source': '..\\\\data\\\\Explorations in Artificial Intelligence and Machine Learning, CRC Press.pdf', 'total_pages': 178.0}, page_content='etc.). To proceed, science needs to better understand what the difference \\nbetween a feeling and a thought is. Feeling pain and knowing about pain')], 'answer': \"I don't know.\"}\n",
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What is headache meaning?\"})\n",
    "print(response)\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b93363c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt engineering is the art and science of crafting effective prompts to guide and control the context and specific goals for a model, ultimately influencing the quality and direction of its outputs. It focuses on specific functionalities and requirements.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What is prompt engineering?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "d0bed61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What are features of Java?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "1397aa5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM stands for Large Language Model. It is a type of artificial intelligence model designed to understand and generate human language based on input data. LLMs are foundational to many generative AI applications.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What is LLm?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4f0505",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
